{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "# verify if GPU is available\n",
    "if torch.cuda.is_available(): \n",
    "    \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize variables\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# mysql credentials\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "USER = os.getenv(\"USER\")\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to db\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"127.0.0.1\",\n",
    "  user=USER,\n",
    "  password=PASSWORD,\n",
    "  database=\"mpp21\"\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"collection\", \"processing\", \"dissemination\", \"risk\", \"protection\"]\n",
    "\n",
    "article_ids = {}\n",
    "\n",
    "for c in categories:\n",
    "    article_ids[c] = {}\n",
    "\n",
    "for c in categories:\n",
    "    with open('./final_annotations/'+ c +'_org.csv', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for r in reader:\n",
    "                article_ids[c][r[0]] = r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dissemination 229\n",
      "protection 248\n",
      "collection 250\n",
      "processing 238\n",
      "risk 200\n"
     ]
    }
   ],
   "source": [
    "for i in article_ids.keys():\n",
    "    print(i, len(article_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get intersection\n",
    "# intersection_over_all = set(article_ids[\"collection\"].keys()).intersection(set(article_ids[\"processing\"].keys()))\n",
    "# intersection_over_all = intersection_over_all.intersection(set(article_ids[\"dissemination\"].keys()))\n",
    "# intersection_over_all = intersection_over_all.intersection(set(article_ids[\"risk\"].keys()))\n",
    "# intersection_over_all = intersection_over_all.intersection(set(article_ids[\"protection\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # also get union for all contents\n",
    "# union_over_all = set(article_ids[\"collection\"].keys()).union(set(article_ids[\"processing\"].keys()))\n",
    "# union_over_all = union_over_all.union(set(article_ids[\"dissemination\"].keys()))\n",
    "# union_over_all = union_over_all.union(set(article_ids[\"risk\"].keys()))\n",
    "# union_over_all = union_over_all.union(set(article_ids[\"protection\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(intersection_over_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the same test set, 20% of the smallest len, here 115\n",
    "# len_test_set = round(0.2 * len(intersection_over_all))\n",
    "# len_validation_set = round(0.1 * (len(intersection_over_all) - len_test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IF ONLY ABSTRACT\n",
    "# db methods\n",
    "def select_article(curs, db, aid):\n",
    "    \n",
    "    curs.execute(\"\"\"SELECT title, content \n",
    "                FROM articles \n",
    "                WHERE article_id = '\"\"\" + aid + \"\"\"' \n",
    "                \"\"\")\n",
    "                #LIMIT 200\"\"\") \n",
    "    \n",
    "    arts = {}\n",
    "    result = curs.fetchall()\n",
    "    for r in result:\n",
    "        arts[aid] = {\n",
    "            \"title\": r[0],\n",
    "            \"content\": r[1],\n",
    "            \"verdict\": article_ids[CLASSIFICATION][aid]\n",
    "        }\n",
    "    \n",
    "    return arts[aid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random \n",
    "# copy_intersection = list(intersection_over_all)\n",
    "# test_set_ids, copy_intersection = get_set(len_test_set, copy_intersection)\n",
    "# validation_set_ids, copy_intersection = get_set(len_validation_set, copy_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# test_set_ids = list(pd.read_csv(\"final_annotations/test_set.csv\")[\"header\"])\n",
    "# validation_set_ids = list(pd.read_csv(\"final_annotations/validation_set.csv\")[\"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_set_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "#     TO DO:\n",
    "#     save test and validation ids when running full thing\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE ROUND OF BINARY CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION = \"risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-239f58c4f88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_set_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_set_ids' is not defined"
     ]
    }
   ],
   "source": [
    "contents = {}\n",
    "\n",
    "for a in article_ids[CLASSIFICATION]:#.keys():\n",
    "    temp = select_article(mycursor, mydb, a)\n",
    "    \n",
    "    contents[a] = {\n",
    "        \"id\": a,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }\n",
    "\n",
    "    \n",
    "# split into three sets\n",
    "\n",
    "training_data = {}\n",
    "validation_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for a in contents.keys():\n",
    "    if a in test_set_ids:\n",
    "        test_data[a] = contents[a]\n",
    "    elif a in validation_set_ids:\n",
    "        validation_data[a] = contents[a]\n",
    "    else:\n",
    "        training_data[a] = contents[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_ids[CLASSIFICATION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(cnt, ids):\n",
    "    \n",
    "    copy = ids\n",
    "    \n",
    "    random_ids = []\n",
    "    \n",
    "#     if len(keys) == 0:\n",
    "#         return []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        choice = random.choice(copy)\n",
    "        random_ids.append(choice)\n",
    "        copy.remove(choice)\n",
    "        \n",
    "    return sorted(random_ids), copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# len_test = round(0.1 * len(article_ids[CLASSIFICATION]))\n",
    "# len_valid = round(0.1 * len(article_ids[CLASSIFICATION]))\n",
    "# len_train = len(article_ids[CLASSIFICATION]) - len_test - len_valid\n",
    "\n",
    "ones = [i for i in article_ids[CLASSIFICATION].keys() if int(article_ids[CLASSIFICATION][i]) == 1]\n",
    "zeroes = [i for i in article_ids[CLASSIFICATION].keys() if int(article_ids[CLASSIFICATION][i]) == 0]\n",
    "print(len(zeroes))\n",
    "LEN_ONES = len(ones)\n",
    "LEN_ZEROES = len(zeroes)\n",
    "\n",
    "test_data, ones = get_set(round(0.1 * LEN_ONES), ones)# get 1s\n",
    "print(len(test_data))\n",
    "temp, zeroes = get_set(round(0.1 * LEN_ZEROES), zeroes) # get 0s\n",
    "for i in temp:\n",
    "    test_data.append(i)\n",
    "    \n",
    "validation_data, ones = get_set(round(0.1 * LEN_ONES), ones)# get 1s\n",
    "print(len(validation_data))\n",
    "temp, zeroes = get_set(round(0.1 * LEN_ZEROES), zeroes) # get 0s\n",
    "for i in temp:\n",
    "    validation_data.append(i)\n",
    "    \n",
    "training_data = ones\n",
    "for i in zeroes:\n",
    "    training_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = {}\n",
    "final_training_data = {}\n",
    "final_validation_data = {}\n",
    "\n",
    "for i in test_data:\n",
    "    temp = select_article(mycursor, mydb, i)\n",
    "    final_test_data[i] = {\n",
    "        \"id\": i,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }\n",
    "    \n",
    "for i in training_data:\n",
    "    temp = select_article(mycursor, mydb, i)\n",
    "    final_training_data[i] = {\n",
    "        \"id\": i,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }\n",
    "    \n",
    "for i in validation_data:\n",
    "    temp = select_article(mycursor, mydb, i)\n",
    "    final_validation_data[i] = {\n",
    "        \"id\": i,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 20\n",
      "Number of validation sentences: 20\n",
      "Number of training sentences: 160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TDP_2459</th>\n",
       "      <td>TDP_2459</td>\n",
       "      <td>0</td>\n",
       "      <td>The thieves of our privacy My family had its s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDP_306</th>\n",
       "      <td>TDP_306</td>\n",
       "      <td>0</td>\n",
       "      <td>Otaki firm goes global to foil cyber-criminals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS_6428</th>\n",
       "      <td>TS_6428</td>\n",
       "      <td>0</td>\n",
       "      <td>Ensuring smart home features don't become a bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NYT_2467</th>\n",
       "      <td>NYT_2467</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't limit speech in France FULL TEXTOn Tuesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMH_6195</th>\n",
       "      <td>SMH_6195</td>\n",
       "      <td>1</td>\n",
       "      <td>AI won't rule humans if we take charge What do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_6069</th>\n",
       "      <td>DT_6069</td>\n",
       "      <td>0</td>\n",
       "      <td>The new digital etiquetttte for t grown-ups Av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDP_1808</th>\n",
       "      <td>TDP_1808</td>\n",
       "      <td>0</td>\n",
       "      <td>Are we all facing an uncertain future? A whiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NYT_1548</th>\n",
       "      <td>NYT_1548</td>\n",
       "      <td>1</td>\n",
       "      <td>GPS and the law FULL TEXTLast week brought fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDP_1667</th>\n",
       "      <td>TDP_1667</td>\n",
       "      <td>0</td>\n",
       "      <td>Hospitals' photo ban a delusion Hospitals must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS_7063</th>\n",
       "      <td>TS_7063</td>\n",
       "      <td>0</td>\n",
       "      <td>Which Ontario workers are most at risk for COV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id label                                               text\n",
       "TDP_2459  TDP_2459     0  The thieves of our privacy My family had its s...\n",
       "TDP_306    TDP_306     0  Otaki firm goes global to foil cyber-criminals...\n",
       "TS_6428    TS_6428     0  Ensuring smart home features don't become a bu...\n",
       "NYT_2467  NYT_2467     0  Don't limit speech in France FULL TEXTOn Tuesd...\n",
       "SMH_6195  SMH_6195     1  AI won't rule humans if we take charge What do...\n",
       "DT_6069    DT_6069     0  The new digital etiquetttte for t grown-ups Av...\n",
       "TDP_1808  TDP_1808     0  Are we all facing an uncertain future? A whiff...\n",
       "NYT_1548  NYT_1548     1  GPS and the law FULL TEXTLast week brought fre...\n",
       "TDP_1667  TDP_1667     0  Hospitals' photo ban a delusion Hospitals must...\n",
       "TS_7063    TS_7063     0  Which Ontario workers are most at risk for COV..."
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame.from_dict(final_test_data).T\n",
    "df_training = pd.DataFrame.from_dict(final_training_data).T\n",
    "df_validation = pd.DataFrame.from_dict(final_validation_data).T\n",
    "\n",
    "print('Number of test sentences: {:,}'.format(df_test.shape[0]))\n",
    "print('Number of validation sentences: {:,}'.format(df_validation.shape[0]))\n",
    "print('Number of training sentences: {:,}'.format(df_training.shape[0]))\n",
    "\n",
    "df_training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[\"id\"].to_csv('final_annotations/test_set.csv', index=False)\n",
    "# df_validation[\"id\"].to_csv('final_annotations/validation_set.csv', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data): # a pandas frame\n",
    "    \n",
    "    sentences = data.text.values\n",
    "    labels = data.label.values\n",
    "    \n",
    "    input_ids = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = 512,\n",
    "                       )\n",
    "\n",
    "\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in input_ids:\n",
    "\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "        attention_masks.append(att_mask)\n",
    "    \n",
    "    return input_ids, labels, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into train and test sets (0.8 training, 0.2 test)\n",
    "# def split_data(inputs, labels, masks, split):\n",
    "\n",
    "#     train_data, test_data, train_labels, test_labels = train_test_split(inputs, labels, \n",
    "#                                                                 random_state=2018, test_size=split)\n",
    "#     # split masks\n",
    "#     train_masks, test_masks, _, _ = train_test_split(masks, labels,\n",
    "#                                                  random_state=2018, test_size=split)\n",
    "    \n",
    "#     return train_data, test_data, train_labels, test_labels, train_masks, test_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids, labels, attention_masks = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels, train_masks = prepare_data(df_training)\n",
    "test_inputs, test_labels, test_masks = prepare_data(df_test)\n",
    "validation_inputs, validation_labels, validation_masks = prepare_data(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast labels # if still needed\n",
    "import numpy as np\n",
    "\n",
    "x, y, z = [], [], []\n",
    "\n",
    "[x.append(i) for i in train_labels]\n",
    "[y.append(i) for i in validation_labels]\n",
    "[z.append(i) for i in test_labels]\n",
    "\n",
    "train_labels = np.array(x)\n",
    "validation_labels = np.array(y)\n",
    "test_labels = np.array(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert inputs and labels into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t\t 160\n",
      "validation\t 20\n",
      "test\t\t 20\n"
     ]
    }
   ],
   "source": [
    "print(\"train\\t\\t\", len(train_inputs))\n",
    "print(\"validation\\t\", len(validation_inputs))\n",
    "print(\"test\\t\\t\", len(test_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16 # 32 #16 # 32\n",
    "\n",
    "# DataLoader for training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# load BertForSequenceClassification (pre-trained BERT model with a single linear classif. layer)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # 12-layer BERT model, uncased vocab (TODO: uncase articles)\n",
    "    # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # number of output labels\n",
    "                        # 2: binary classification\n",
    "                        # >2: multi-class   \n",
    "    output_attentions = False, # return attention weights\n",
    "    output_hidden_states = False, # return hidden states\n",
    ")\n",
    "\n",
    "# run on GPU (uncomment if GPU available on server)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# get model's parameters\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5,\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# training epochs: recommended 2-4\n",
    "epochs = 15#10#0 ## CHANGE TO 4 - 8 - 10\n",
    "\n",
    "# training steps = batches * epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            # default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy of predictions\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "   \n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'risk'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epoch took: 0:03:08\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.78125\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epoch took: 0:03:07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.78125\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoch took: 0:03:08\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75000\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.46875\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.50000\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:03:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.56250\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:03:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.59375\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40625\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40625\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40625\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:04\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.37500\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:02\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40625\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:04\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.37500\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:01\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40625\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40625\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# store average loss after each epoch\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "models = {}\n",
    "stats = {}\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # full pass over the training set    \n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')    \n",
    "    t0 = time.time() # measure epoch length\n",
    "    total_loss = 0 # reset total loss for current epoch\n",
    "    \n",
    "    # place model in training mode\n",
    "    model.train()    \n",
    "\n",
    "    # for each batch\n",
    "    for step, batch in enumerate(train_dataloader):        \n",
    "        #print(\"Step: \", step)\n",
    "        # updated on progress every 40 batches\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0) # compute elapsed time\n",
    "            \n",
    "            # report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))        \n",
    "            \n",
    "        # unpack training batch from DataLoader\n",
    "        # copy each tensor to GPU ('to' method)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)        \n",
    "        \n",
    "        # clear previously calculated gradients before a backward pass\n",
    "        model.zero_grad()                \n",
    "        \n",
    "        # forward pass (i.e., evaluate the model on this training batch)\n",
    "        # this returns the loss (and not the model output), because we provided the `labels`\n",
    "\n",
    "        # documentation for model function:\n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # get loss value\n",
    "        loss = outputs[0]        \n",
    "        \n",
    "        # accumulate training loss over all batches (to compute average loss at the end)\n",
    "        total_loss += loss.item()     \n",
    "        \n",
    "        # backward pass to calculate gradients\n",
    "        loss.backward()        \n",
    "        \n",
    "        # clip the norm of the gradients to 1.0 to prevent the 'exploding gradients' problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)        \n",
    "        \n",
    "        # update parameters (the optimizer dictates the update rule based on gradients, learning rate, etc.)\n",
    "        # take a step using the computed gradient\n",
    "        optimizer.step() \n",
    "        \n",
    "        # update the learning rate.\n",
    "        scheduler.step()    \n",
    "    \n",
    "    # calculate average loss over training data\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # store the loss value to plot the learning curve\n",
    "    loss_values.append(avg_train_loss)    \n",
    "    \n",
    "    print()\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    # after each training epoch, measure performance on validation set\n",
    "    print()\n",
    "    print(\"Running Validation...\")    \n",
    "    t0 = time.time()   \n",
    "    \n",
    "    ### STORE MODEL\n",
    "    models[epoch_i] = model\n",
    "    fn = \"models/\"+CLASSIFICATION+\"/model_r2_e\" + str(epoch_i) + \".sav\"\n",
    "    pickle.dump(model, open(fn, 'wb'))\n",
    "    \n",
    "    # place model in evaluation mode (dropout layers behave differently than during training)\n",
    "    model.eval()    \n",
    "    \n",
    "    # tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0    \n",
    "    \n",
    "    # evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # unpack inputs from DataLoader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        \n",
    "        # do not compute or store gradients to save memory and speedup validation\n",
    "        with torch.no_grad():                    \n",
    "            # forward pass, calculate logit predictions\n",
    "                # will return logits (not the loss, because we have not provided labels)\n",
    "            # token_type_ids = segment ids (differentiates between sentence 1 and 2 in 2-sentence tasks)\n",
    "            \n",
    "            \n",
    "            # documentation for model\n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # get logits (output) from model\n",
    "        # values prior to applying an activation function (e.g., softmax)\n",
    "        logits = outputs[0]    \n",
    "        \n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # compute accuracy for current batch \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # accumulate accuracy\n",
    "        eval_accuracy += tmp_eval_accuracy        \n",
    "        # track batches\n",
    "        nb_eval_steps += 1    \n",
    "        # report final accuracy for current validation run\n",
    "    print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    stats[epoch_i] = {\n",
    "        \"acc\": round(eval_accuracy/nb_eval_steps, 5),\n",
    "        \"avg-loss\": round(avg_train_loss, 5)\n",
    "    }\n",
    "    print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>avg-loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.55114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.52356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.44836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.34731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.24083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.56250</td>\n",
       "      <td>0.16246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.59375</td>\n",
       "      <td>0.11350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.06308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.04362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.03264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.02650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.02145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.01776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.01866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.01556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc  avg-loss\n",
       "0   0.78125   0.55114\n",
       "1   0.78125   0.52356\n",
       "2   0.75000   0.44836\n",
       "3   0.46875   0.34731\n",
       "4   0.50000   0.24083\n",
       "5   0.56250   0.16246\n",
       "6   0.59375   0.11350\n",
       "7   0.40625   0.06308\n",
       "8   0.40625   0.04362\n",
       "9   0.40625   0.03264\n",
       "10  0.37500   0.02650\n",
       "11  0.40625   0.02145\n",
       "12  0.37500   0.01776\n",
       "13  0.40625   0.01866\n",
       "14  0.40625   0.01556"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats = pd.DataFrame.from_dict(stats)\n",
    "df_stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_model(m):\n",
    "    return pickle.load(open(\"models/\"+CLASSIFICATION+\"/model_r2_e\" + str(m) + \".sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(14) # CHANGE NUMBER (uses index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 20 test documents...\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# prediction on test set\n",
    "print('Predicting labels for {:,} test documents...'.format(len(test_inputs)))\n",
    "\n",
    "# place model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# tracking variables \n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# predict \n",
    "for batch in test_dataloader:\n",
    "    # add batch to GPU (if available)\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "    # unpack inputs from DataLoader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        # forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask) \n",
    "        \n",
    "    logits = outputs[0]  \n",
    "    \n",
    "    # move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "    \n",
    "print('\\nDONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# for each batch\n",
    "for i in range(len(true_labels)):\n",
    "    # predictions are a 2-column ndarray (one for 0, one for 1)\n",
    "    # pick label with highest value and turn it into 1 or 0\n",
    "\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "    # calculate and store coef for current batch\n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: -0.367\n"
     ]
    }
   ],
   "source": [
    "# combine the predictions for each batch into a single list of 0s and 1s\n",
    "\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# combine the correct labels for each batch into a single list\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TN: 9 \n",
      "TP: 0 \n",
      "FN: 4 \n",
      "FP: 7\n",
      "Recall: 0.281\n",
      "Precision: 0.346\n",
      "F1-score: 0.31\n",
      "0.28125\n",
      "FPR [0.     0.4375 1.    ]\n",
      "TPR [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "x_data = flat_true_labels\n",
    "y_data = flat_predictions\n",
    "\n",
    "tn, fp, fn, tp = get_conf_matrix(x_data, y_data)#confusion_matrix(np.array(flat_true_labels), flat_predictions).ravel()\n",
    "print(\n",
    "    \"\\nTN:\", tn,\n",
    "    \"\\nTP:\", tp,\n",
    "    \"\\nFN:\", fn,\n",
    "    \"\\nFP:\", fp    \n",
    ")\n",
    "\n",
    "recall = round(get_recall(x_data, y_data), 3)\n",
    "precision = round(get_precision(x_data, y_data), 3)\n",
    "f1 = round(get_f1_score(x_data, y_data), 3)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-score:\", f1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(x_data, y_data, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "print(\"FPR\", fpr)\n",
    "print(\"TPR\", tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def get_conf_matrix(x, y):\n",
    "    return confusion_matrix(np.array(x), y).ravel()\n",
    "\n",
    "def get_recall(x, y):\n",
    "    return recall_score(x, y, average='macro')\n",
    "    \n",
    "def get_precision(x, y):\n",
    "    return precision_score(x, y, average='macro')\n",
    "\n",
    "def get_f1_score(x, y):\n",
    "    return f1_score(x, y, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: predict function for the 26k articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_articles_for_prediction(curs, db, n):\n",
    "    \n",
    "    curs.execute(\"\"\"SELECT article_id, title, content \n",
    "                FROM articles \n",
    "                WHERE newspaper = '\"\"\" + n + \"\"\"' \n",
    "                AND is_privacy = 'privacy'\n",
    "                \"\"\")\n",
    "                #LIMIT 200\"\"\") AND year(DATE) != '2010'\n",
    "    \n",
    "    arts = {}\n",
    "    result = curs.fetchall()\n",
    "    for r in result:\n",
    "        arts[r[0]] = {\n",
    "            \"title\": r[1],\n",
    "            \"content\": r[2]\n",
    "        }\n",
    "    \n",
    "    return arts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_content(d):\n",
    "    # tokenize\n",
    "    encoded_sent = tokenizer.encode(d, add_special_tokens = True, max_length = 512)\n",
    "    \n",
    "    # pad\n",
    "    input_id = pad_sequences([encoded_sent], maxlen=512, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    #print(input_id)\n",
    "    \n",
    "    # create attention mask\n",
    "    seq_mask = [float(i>0) for i in input_id[0]]\n",
    "\n",
    "    # convert to tensors\n",
    "    prediction_input = torch.tensor(input_id)\n",
    "    prediction_mask = torch.tensor([seq_mask])\n",
    "    \n",
    "    res = {\n",
    "        \"input\": prediction_input,\n",
    "        \"mask\": prediction_mask\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "# function to get category label\n",
    "def is_category(res):\n",
    "    \n",
    "#     # tokenize\n",
    "#     encoded_sent = tokenizer.encode(d, add_special_tokens = True, max_length = 512)\n",
    "    \n",
    "#     # pad\n",
    "#     input_id = pad_sequences([encoded_sent], maxlen=512, \n",
    "#                           dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "#     #print(input_id)\n",
    "    \n",
    "#     # create attention mask\n",
    "#     seq_mask = [float(i>0) for i in input_id[0]]\n",
    "\n",
    "#     # convert to tensors\n",
    "#     prediction_input = torch.tensor(input_id)\n",
    "#     prediction_mask = torch.tensor([seq_mask])\n",
    "    output = MODEL(res[\"input\"], token_type_ids=None, \n",
    "                      attention_mask=res[\"mask\"]) \n",
    "        \n",
    "    logits = output[0]  \n",
    "    \n",
    "    # move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    # store predictions and true labels\n",
    "    verdict = np.argmax(logits[0])\n",
    "\n",
    "    return int(verdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = \"TG\"\n",
    "articles_to_predict = select_articles_for_prediction(mycursor, mydb, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7435"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7435\n"
     ]
    }
   ],
   "source": [
    "contents_to_predict = {}\n",
    "\n",
    "for a in articles_to_predict.keys():\n",
    "    \n",
    "    contents_to_predict[a] = {\n",
    "        \"id\": a,\n",
    "        \"text\": articles_to_predict[a][\"title\"] + \" \" + articles_to_predict[a][\"content\"]\n",
    "    }\n",
    "\n",
    "print(len(contents_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7435\n"
     ]
    }
   ],
   "source": [
    "parsed_contents = {}\n",
    "\n",
    "for i in contents_to_predict.keys():\n",
    "    parsed_contents[i] = parse_content(contents_to_predict[i][\"text\"])\n",
    "print(len(parsed_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7435\n"
     ]
    }
   ],
   "source": [
    "verdicts = {}\n",
    "\n",
    "for i in parsed_contents.keys():\n",
    "    verdicts[i] = {\n",
    "      #  \"collection\": -1,\n",
    "      #  \"processing\": -1,\n",
    "        CLASSIFICATION: -1\n",
    "    }\n",
    "print(len(verdicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: 02: 45 PM\n",
      "Predicted: 300 Left: 7135\n",
      "20: 04: 15 PM\n",
      "Predicted: 600 Left: 6835\n",
      "20: 05: 45 PM\n",
      "Predicted: 900 Left: 6535\n",
      "20: 07: 15 PM\n",
      "Predicted: 1200 Left: 6235\n",
      "20: 08: 46 PM\n",
      "Predicted: 1500 Left: 5935\n",
      "20: 10: 14 PM\n",
      "Predicted: 1800 Left: 5635\n",
      "20: 11: 40 PM\n",
      "Predicted: 2100 Left: 5335\n",
      "20: 13: 14 PM\n",
      "Predicted: 2400 Left: 5035\n",
      "20: 14: 42 PM\n",
      "Predicted: 2700 Left: 4735\n",
      "20: 16: 12 PM\n",
      "Predicted: 3000 Left: 4435\n",
      "20: 17: 41 PM\n",
      "Predicted: 3300 Left: 4135\n",
      "20: 19: 10 PM\n",
      "Predicted: 3600 Left: 3835\n",
      "20: 20: 36 PM\n",
      "Predicted: 3900 Left: 3535\n",
      "20: 22: 06 PM\n",
      "Predicted: 4200 Left: 3235\n",
      "20: 23: 35 PM\n",
      "Predicted: 4500 Left: 2935\n",
      "20: 25: 04 PM\n",
      "Predicted: 4800 Left: 2635\n",
      "20: 26: 34 PM\n",
      "Predicted: 5100 Left: 2335\n",
      "20: 28: 02 PM\n",
      "Predicted: 5400 Left: 2035\n",
      "20: 29: 32 PM\n",
      "Predicted: 5700 Left: 1735\n",
      "20: 31: 01 PM\n",
      "Predicted: 6000 Left: 1435\n",
      "20: 32: 29 PM\n",
      "Predicted: 6300 Left: 1135\n",
      "20: 33: 58 PM\n",
      "Predicted: 6600 Left: 835\n",
      "20: 35: 27 PM\n",
      "Predicted: 6900 Left: 535\n",
      "20: 36: 57 PM\n",
      "Predicted: 7200 Left: 235\n",
      "20: 38: 26 PM\n",
      "20: 39: 32 PM\n"
     ]
    }
   ],
   "source": [
    "categ = 0\n",
    "non_categ = 0\n",
    "ctr = 0\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "print(now.strftime('%H: %M: %S %p'))\n",
    "\n",
    "for i in parsed_contents.keys():\n",
    "    v = is_category(parsed_contents[i])\n",
    "    verdicts[i][CLASSIFICATION] = v\n",
    "    \n",
    "    ctr += 1\n",
    "    if ctr % 300 == 0:\n",
    "        print(\"Predicted:\", ctr, \"Left:\", len(parsed_contents) - ctr)\n",
    "        now = datetime.now()\n",
    "\n",
    "        print(now.strftime('%H: %M: %S %p'))\n",
    "    if v == 1:\n",
    "        categ += 1\n",
    "    elif v == 0:\n",
    "        non_categ += 1\n",
    "        \n",
    "now = datetime.now()\n",
    "\n",
    "print(now.strftime('%H: %M: %S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dump verdicts to update all at once\n",
    "import json\n",
    "with open(\"verdicts/\"+ CLASSIFICATION +\"_\"+ n +\".json\", \"w\") as outfile:  \n",
    "    json.dump(verdicts, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categ: 1484\n",
      "19.96\n",
      "Non-categ: 5951\n",
      "80.04\n"
     ]
    }
   ],
   "source": [
    "print(\"Categ:\", categ)\n",
    "print(round(categ * 100 / len(parsed_contents.keys()), 2))\n",
    "\n",
    "print(\"Non-categ:\", non_categ)\n",
    "print(round(non_categ * 100 / len(parsed_contents.keys()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL USED\n",
    "#pickle.dump(MODEL, open(\"models/final_\"+ CLASSIFICATION +\".sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in verdicts.keys():\n",
    "#     update_categ(mycursor, mydb, json.dumps(verdicts[a]), a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
