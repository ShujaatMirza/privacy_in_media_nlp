{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import time\n",
    "from datetime import datetime \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    " \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize variables\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# mysql credentials\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "USER = os.getenv(\"USER\")\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to db\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"127.0.0.1\",\n",
    "  user=USER,\n",
    "  password=PASSWORD,\n",
    "  database=\"mpp21\"\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer(stop_words='english') # count_vectorizer\n",
    "NUMBER_TOPICS = 1\n",
    "NUMBER_TOPICS2 = 10\n",
    "MODEL_WORDS = 50 #50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(cnt):\n",
    "    frequencies = vector.fit_transform(cnt)\n",
    "\n",
    "    lda = LDA(n_components=NUMBER_TOPICS, n_jobs=-1, verbose=0)\n",
    "    lda.fit(frequencies) # extract topic\n",
    "\n",
    "    words = vector.get_feature_names()\n",
    "\n",
    "    for topicID, topic in enumerate(lda.components_):\n",
    "        topics = [words[i] for i in topic.argsort()[:-MODEL_WORDS - 1:-1]] \n",
    "        \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"collection\", \"processing\", \"dissemination\", \"risk\", \"protection\"]\n",
    "\n",
    "article_ids = {}\n",
    "\n",
    "for c in categories:\n",
    "    article_ids[c] = {}\n",
    "\n",
    "for c in categories:\n",
    "    with open('./final_annotations/'+ c +'_org.csv', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for r in reader:\n",
    "                article_ids[c][r[0]] = r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION = \"risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyterms = [\n",
    "#     \"aggregat\", \"algorithm\", \"analy\", \"computing\", \"extract\", \"identif\", \"linking\", \"mapping\",\n",
    "#     \"mining\", \"predict\", \"processed\", \"processing\"\n",
    "    \n",
    "# ]\n",
    "\n",
    "# keyterms = [\n",
    "    \n",
    "#     \"amendment\", \"anonym\", \"confidential\", \"consent\", \"encrypt\", \"framework\", \"guideline\",\n",
    "#     \"legal\", \"legislat\", \"policy\",\n",
    "# \"protect\", \"regul\", \"ruling\", \"safe\", \"standard\", \"vpn\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_set(cnt, ids):\n",
    "    \n",
    "    copy = ids\n",
    "    \n",
    "    random_ids = []\n",
    "    \n",
    "#     if len(keys) == 0:\n",
    "#         return []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        choice = random.choice(copy)\n",
    "        random_ids.append(choice)\n",
    "        copy.remove(choice)\n",
    "        \n",
    "    return sorted(random_ids), copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_test = round(0.1 * len(article_ids[CLASSIFICATION]))\n",
    "# len_valid = round(0.1 * len(article_ids[CLASSIFICATION]))\n",
    "# len_train = len(article_ids[CLASSIFICATION]) - len_test - len_valid\n",
    "\n",
    "ones = [i for i in article_ids[CLASSIFICATION].keys() if int(article_ids[CLASSIFICATION][i]) == 1]\n",
    "COPY_ONES = ones\n",
    "zeroes = [i for i in article_ids[CLASSIFICATION].keys() if int(article_ids[CLASSIFICATION][i]) == 0]\n",
    "#print(len(zeroes))\n",
    "LEN_ONES = len(ones)\n",
    "LEN_ZEROES = len(zeroes)\n",
    "\n",
    "test_data, ones = get_set(round(0.1 * LEN_ONES), ones)# get 1s\n",
    "#print(len(test_data))\n",
    "temp, zeroes = get_set(round(0.1 * LEN_ZEROES), zeroes) # get 0s\n",
    "for i in temp:\n",
    "    test_data.append(i)\n",
    "    \n",
    "validation_data, ones = get_set(round(0.1 * LEN_ONES), ones)# get 1s\n",
    "#print(len(validation_data))\n",
    "temp, zeroes = get_set(round(0.1 * LEN_ZEROES), zeroes) # get 0s\n",
    "for i in temp:\n",
    "    validation_data.append(i)\n",
    "    \n",
    "training_data = ones\n",
    "for i in zeroes:\n",
    "    training_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IF ONLY ABSTRACT\n",
    "# db methods\n",
    "def select_article(curs, db, aid):\n",
    "    \n",
    "    curs.execute(\"\"\"SELECT title, content \n",
    "                FROM articles \n",
    "                WHERE article_id = '\"\"\" + aid + \"\"\"' \n",
    "                \"\"\")\n",
    "                #LIMIT 200\"\"\") \n",
    "    \n",
    "    arts = {}\n",
    "    result = curs.fetchall()\n",
    "    for r in result:\n",
    "        arts[aid] = {\n",
    "            \"title\": r[0],\n",
    "            \"content\": r[1],\n",
    "            \"verdict\": article_ids[CLASSIFICATION][aid]\n",
    "        }\n",
    "    \n",
    "    return arts[aid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = {}\n",
    "final_training_data = {}\n",
    "final_validation_data = {}\n",
    "\n",
    "for i in test_data:\n",
    "    temp = select_article(mycursor, mydb, i)\n",
    "    final_test_data[i] = {\n",
    "        \"id\": i,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }\n",
    "    \n",
    "for i in training_data:\n",
    "    temp = select_article(mycursor, mydb, i)\n",
    "    final_training_data[i] = {\n",
    "        \"id\": i,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }\n",
    "    \n",
    "for i in validation_data:\n",
    "    temp = select_article(mycursor, mydb, i)\n",
    "    final_validation_data[i] = {\n",
    "        \"id\": i,\n",
    "        \"label\": int(temp[\"verdict\"]),\n",
    "        \"text\": temp[\"title\"] + \" \" + temp[\"content\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 20\n",
      "Number of validation sentences: 20\n",
      "Number of training sentences: 160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT_4614</th>\n",
       "      <td>DT_4614</td>\n",
       "      <td>0</td>\n",
       "      <td>Voters are more predictable than they think Ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMH_956</th>\n",
       "      <td>SMH_956</td>\n",
       "      <td>0</td>\n",
       "      <td>Immigration edict leaves child humiliated at g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMH_2358</th>\n",
       "      <td>SMH_2358</td>\n",
       "      <td>0</td>\n",
       "      <td>Revenge porn: caught in a web of spite Hundred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDP_716</th>\n",
       "      <td>TDP_716</td>\n",
       "      <td>1</td>\n",
       "      <td>Adverts by app - Dotcom looks to future KIM DO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMH_2236</th>\n",
       "      <td>SMH_2236</td>\n",
       "      <td>1</td>\n",
       "      <td>Pub scanners raise privacy concerns More than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS_4155</th>\n",
       "      <td>TS_4155</td>\n",
       "      <td>0</td>\n",
       "      <td>Your face could be your next password;Shopping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TG_13948</th>\n",
       "      <td>TG_13948</td>\n",
       "      <td>0</td>\n",
       "      <td>A historic antitrust hearing in Congress has p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_1346</th>\n",
       "      <td>AFR_1346</td>\n",
       "      <td>0</td>\n",
       "      <td>Intrusive privacy breach Your item (\"ABS slamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TG_7270</th>\n",
       "      <td>TG_7270</td>\n",
       "      <td>1</td>\n",
       "      <td>Police face new ethical dilemma in increasingl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TG_8142</th>\n",
       "      <td>TG_8142</td>\n",
       "      <td>1</td>\n",
       "      <td>The injunction is back: entertainer blocks ext...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id label                                               text\n",
       "DT_4614    DT_4614     0  Voters are more predictable than they think Ap...\n",
       "SMH_956    SMH_956     0  Immigration edict leaves child humiliated at g...\n",
       "SMH_2358  SMH_2358     0  Revenge porn: caught in a web of spite Hundred...\n",
       "TDP_716    TDP_716     1  Adverts by app - Dotcom looks to future KIM DO...\n",
       "SMH_2236  SMH_2236     1  Pub scanners raise privacy concerns More than ...\n",
       "TS_4155    TS_4155     0  Your face could be your next password;Shopping...\n",
       "TG_13948  TG_13948     0  A historic antitrust hearing in Congress has p...\n",
       "AFR_1346  AFR_1346     0  Intrusive privacy breach Your item (\"ABS slamm...\n",
       "TG_7270    TG_7270     1  Police face new ethical dilemma in increasingl...\n",
       "TG_8142    TG_8142     1  The injunction is back: entertainer blocks ext..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame.from_dict(final_test_data).T\n",
    "df_training = pd.DataFrame.from_dict(final_training_data).T\n",
    "df_validation = pd.DataFrame.from_dict(final_validation_data).T\n",
    "\n",
    "print('Number of test sentences: {:,}'.format(df_test.shape[0]))\n",
    "print('Number of validation sentences: {:,}'.format(df_validation.shape[0]))\n",
    "print('Number of training sentences: {:,}'.format(df_training.shape[0]))\n",
    "\n",
    "df_training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(m):\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    \n",
    "    for w in m: # for each word in model\n",
    "        for k in keyterms: # for each keyterm in a category\n",
    "            if k in w[:len(k)]: # get only the words that begin with\n",
    "                words.append({\n",
    "                     \"keyword\": k, \n",
    "                     \"model_word\": w\n",
    "                 })\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyterms =[ \"cloud\", \"collect\", \"databas\", \"harvest\", \"logging\", \"monitor\", \"record\", \n",
    "\"retention\", \"snoop\", \"storage\", \"stored\", \"surveil\",\n",
    "\"track\"]\n",
    "\n",
    "# keyterms =[\"access\",\"attack\",\"breach\",\"disclos\",\"hack\",\"intercept\",\"intrud\",\"intrus\",\"leak\",\n",
    "# \"risk\",\"snoop\",\"spy\",\"spying\",\"stalk\",\"surveil\",\"theft\",\"track\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def get_conf_matrix(x, y):\n",
    "    return confusion_matrix(np.array(x), y).ravel()\n",
    "\n",
    "def get_recall(x, y):\n",
    "    return recall_score(x, y, average='macro')\n",
    "    \n",
    "def get_precision(x, y):\n",
    "    return precision_score(x, y, average='macro')\n",
    "\n",
    "def get_f1_score(x, y):\n",
    "    return f1_score(x, y, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET = final_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = {}\n",
    "\n",
    "for a in SET.keys():\n",
    "    true_labels[a] = SET[a][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = {}\n",
    "models = {}\n",
    "frqs = {}\n",
    "\n",
    "for a in SET.keys():\n",
    "    m = model([SET[a][\"text\"]])\n",
    "    models[a] = m\n",
    "    f = get_frequencies(m)\n",
    "    frqs[a] = f\n",
    "    isc = is_category(f)\n",
    "    predicted_labels[a] = isc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TN: 148 \n",
      "TP: 9 \n",
      "FN: 43 \n",
      "FP: 0\n",
      "Recall: 0.587\n",
      "Precision: 0.887\n",
      "F1-score: 0.584\n"
     ]
    }
   ],
   "source": [
    "ids = sorted(true_labels.keys())\n",
    "\n",
    "x_data = [true_labels[i] for i in ids]\n",
    "y_data = [predicted_labels[i] for i in ids]\n",
    "\n",
    "tn, fp, fn, tp = get_conf_matrix(x_data, y_data)\n",
    "print(\n",
    "    \"\\nTN:\", tn,\n",
    "    \"\\nTP:\", tp,\n",
    "    \"\\nFN:\", fn,\n",
    "    \"\\nFP:\", fp    \n",
    ")\n",
    "\n",
    "recall = round(get_recall(x_data, y_data), 3)\n",
    "precision = round(get_precision(x_data, y_data), 3)\n",
    "f1 = round(get_f1_score(x_data, y_data), 3)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_category(frq): # based on frequency\n",
    "    \n",
    "    if len(frq) >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(cnt):\n",
    "    frequencies = vector.fit_transform(cnt)\n",
    "\n",
    "    lda = LDA(n_components=NUMBER_TOPICS2, n_jobs=-1, verbose=0)\n",
    "    lda.fit(frequencies) # extract topic\n",
    "\n",
    "    words = vector.get_feature_names()\n",
    "\n",
    "    topics = []\n",
    "    \n",
    "    for topicID, topic in enumerate(lda.components_):\n",
    "        t = [words[i] for i in topic.argsort()[:-MODEL_WORDS - 1:-1]] \n",
    "        for i in t:\n",
    "            topics.append(i)\n",
    "        \n",
    "    return list(set(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = {}\n",
    "models = {}\n",
    "frqs = {}\n",
    "\n",
    "for a in SET.keys():\n",
    "    m = model2([SET[a][\"text\"]])\n",
    "    models[a] = m\n",
    "    f = get_frequencies(m)\n",
    "    frqs[a] = f\n",
    "    isc = is_category(f)\n",
    "    predicted_labels[a] = isc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TN: 147 \n",
      "TP: 10 \n",
      "FN: 42 \n",
      "FP: 1\n",
      "Recall: 0.593\n",
      "Precision: 0.843\n",
      "F1-score: 0.595\n"
     ]
    }
   ],
   "source": [
    "ids = sorted(true_labels.keys())\n",
    "\n",
    "x_data = [true_labels[i] for i in ids]\n",
    "y_data = [predicted_labels[i] for i in ids]\n",
    "\n",
    "tn, fp, fn, tp = get_conf_matrix(x_data, y_data)\n",
    "print(\n",
    "    \"\\nTN:\", tn,\n",
    "    \"\\nTP:\", tp,\n",
    "    \"\\nFN:\", fn,\n",
    "    \"\\nFP:\", fp    \n",
    ")\n",
    "\n",
    "recall = round(get_recall(x_data, y_data), 3)\n",
    "precision = round(get_precision(x_data, y_data), 3)\n",
    "f1 = round(get_f1_score(x_data, y_data), 3)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect keywords from the training set\n",
    "\n",
    "# keywords_from = final_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_contents = {} # all articles in training data\n",
    "# ctr = 0\n",
    "# for a in keywords_from.keys():\n",
    "#     training_contents[ctr] = keywords_from[a][\"text\"]\n",
    "#     ctr += 1\n",
    "\n",
    "# #temp = vector.fit_transform([keywords_from[\"AFR_1004\"][\"text\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df = pd.DataFrame.from_dict(training_contents, orient=\"index\", columns=[\"Article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ids = []\n",
    "\n",
    "with open('./rp.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for r in reader:\n",
    "            rp_ids.append(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IF ONLY ABSTRACT\n",
    "# db methods\n",
    "def select_cnt(curs, db, aid):\n",
    "    \n",
    "    curs.execute(\"\"\"SELECT title, content \n",
    "                FROM articles \n",
    "                WHERE article_id = '\"\"\" + aid + \"\"\"' \n",
    "                \"\"\")\n",
    "                #LIMIT 200\"\"\") \n",
    "    \n",
    "    arts = {}\n",
    "    result = curs.fetchall()\n",
    "    for r in result:\n",
    "        arts[aid] = {\n",
    "            \"content\": r[0] + \" \" + r[1]\n",
    "        }\n",
    "    \n",
    "    return arts[aid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ones only\n",
    "training = {}\n",
    "ctr = 0\n",
    "for a in rp_ids:\n",
    "    training[ctr] = select_cnt(mycursor, mydb, a)[\"content\"]\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "training_df = pd.DataFrame.from_dict(training, orient=\"index\", columns=[\"Article\"])\n",
    "print(training_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Web laws are bad for Google - and us FULL TEXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'PEOPLE IN CRISIS' REPORT;Equip police officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Blagojevich starts his prison term today \"I'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>The injunction is back: entertainer blocks ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Royals awarded damages over topless photos FRA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Article\n",
       "26  Web laws are bad for Google - and us FULL TEXT...\n",
       "11  'PEOPLE IN CRISIS' REPORT;Equip police officer...\n",
       "69  Blagojevich starts his prison term today \"I'll...\n",
       "53  The injunction is back: entertainer blocks ext...\n",
       "44  Royals awarded damages over topless photos FRA..."
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df = 0.80, min_df = 5, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fit = cv.fit_transform(training_df.Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71x812 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7339 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components = 2, max_iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=20,\n",
       "                          mean_change_tol=0.001, n_components=2, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(cv_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_ids[CLASSIFICATION].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 topics 0\n",
      "['access', 'according', 'act', 'action', 'actions', 'added', 'administration', 'agency', 'ago', 'air', 'asked', 'attacks', 'australia', 'australian', 'based', 'believe', 'better', 'big', 'book', 'box', 'breach', 'breaches', 'british', 'business', 'called', 'calls', 'canada', 'car', 'case', 'cent', 'changes', 'citizens', 'civil', 'collection', 'comment', 'commission', 'commissioner', 'committee', 'communications', 'companies', 'company', 'complaint', 'complaints', 'concerns', 'conduct', 'consent', 'consumer', 'control', 'countries', 'country', 'data', 'day', 'department', 'details', 'did', 'digital', 'don', 'drivers', 'driving', 'electronic', 'ethics', 'europe', 'european', 'facebook', 'far', 'federal', 'firm', 'foreign', 'free', 'freedom', 'general', 'giant', 'given', 'global', 'going', 'good', 'google', 'government', 'governments', 'gps', 'group', 'help', 'human', 'idea', 'including', 'independent', 'industry', 'information', 'inquiry', 'insurance', 'intelligence', 'international', 'internet', 'issue', 'issues', 'just', 'justice', 'know', 'law', 'laws', 'legal', 'letter', 'like', 'long', 'make', 'media', 'members', 'million', 'minister', 'money', 'month', 'months', 'mr', 'national', 'need', 'new', 'news', 'obama', 'online', 'operations', 'pay', 'people', 'personal', 'phone', 'place', 'plan', 'policy', 'potential', 'president', 'press', 'prime', 'problem', 'process', 'professor', 'program', 'proposed', 'protection', 'provide', 'public', 'questions', 'radio', 'rate', 'regulation', 'regulators', 'released', 'report', 'reports', 'requests', 'require', 'research', 'right', 'rights', 'risk', 'role', 'rules', 'safety', 'said', 'say', 'scandal', 'search', 'security', 'service', 'services', 'social', 'society', 'software', 'spokesman', 'standards', 'state', 'statement', 'states', 'support', 'surveillance', 'technology', 'think', 'time', 'times', 'told', 'trade', 'twitter', 'union', 'united', 'university', 'use', 'used', 'user', 'users', 'vehicle', 'vehicles', 'want', 'warned', 'way', 'wednesday', 'week', 'work', 'workers', 'world', 'year', 'years', 'yesterday']\n",
      "\n",
      "top 50 topics 1\n",
      "['000', '10', '100', 'access', 'account', 'act', 'allow', 'amazon', 'app', 'apps', 'ask', 'asked', 'association', 'basic', 'best', 'better', 'board', 'body', 'business', 'camera', 'cameras', 'canada', 'care', 'case', 'cases', 'cent', 'chief', 'children', 'code', 'come', 'commissioner', 'common', 'community', 'companies', 'company', 'computers', 'concern', 'concerns', 'consumers', 'content', 'couple', 'court', 'credit', 'customers', 'data', 'day', 'days', 'decision', 'details', 'device', 'devices', 'did', 'different', 'digital', 'documents', 'doing', 'don', 'email', 'end', 'ensure', 'ethics', 'evidence', 'example', 'executive', 'face', 'family', 'force', 'free', 'friends', 'goes', 'going', 'google', 'government', 'hard', 'health', 'high', 'home', 'illegal', 'images', 'important', 'including', 'individual', 'information', 'instead', 'internet', 'isn', 'issue', 'judge', 'just', 'justice', 'kids', 'know', 'known', 'law', 'lawyers', 'let', 'life', 'like', 'likely', 'long', 'look', 'major', 'make', 'making', 'march', 'market', 'media', 'mr', 'ms', 'nearly', 'need', 'new', 'number', 'office', 'officer', 'officers', 'online', 'ontario', 'open', 'parents', 'people', 'person', 'personal', 'phone', 'photo', 'photographs', 'photos', 'place', 'police', 'private', 'probably', 'program', 'project', 'provide', 'provincial', 'public', 'questions', 'real', 'really', 'recent', 'recommendations', 'record', 'recorded', 'recording', 'records', 'report', 'reports', 'request', 'return', 'right', 'risk', 'safety', 'said', 'say', 'says', 'search', 'secure', 'security', 'sense', 'sent', 'seriously', 'service', 'services', 'set', 'sexual', 'share', 'sharing', 'similar', 'simply', 'site', 'smart', 'smartphones', 'start', 'state', 'stored', 'survey', 'systems', 'taking', 'tech', 'technology', 'things', 'think', 'time', 'today', 'told', 'tool', 'toronto', 'use', 'used', 'users', 'using', 'video', 'voice', 'want', 'way', 'website', 'week', 'world', 'year', 'years']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ind, topic in enumerate(lda.components_):\n",
    "    \n",
    "    print(\"top 50 topics\", ind)\n",
    "    top_50 = topic.argsort()[-200:]\n",
    "    print(sorted([feature[i] for i in top_50]))\n",
    "    print()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Article    Local investors stick with Facebook Local fund...\n",
       "Name: AFR_2227, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.T[\"AFR_2227\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x231 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 231 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for topicID, topic in enumerate(lda.components_):\n",
    "    topics = [words[i] for i in topic.argsort()[:-MODEL_WORDS - 1:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
